## Benchmarks

My execution of the benchmarks defined in the source [repo](https://github.com/pvillela/rust-latency-trace/tree/main) on my laptop indicates that there is great variability in the latency tracing overhead. With different target functions, the overhead per span varied from less than 1 µs up to 32 µs. The overhead percentage relative to the total latency of the uninstrumented version of the target function varied from less than 1% to 8%. Furthermore, given a target function, multiple runs of the same Divan or Criterion bencmark can produce significantly varying results. There is so much variability across runs that sometimes nonsensical negative overheads are observed (i.e., the median total latency of an instrumented function is less than that of the uninstrumented version of the same function).
